{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Q-Learning Example: \n",
    "Find shortest number of steps to user defined goal node (simple shortest path problem) from selected start node.\n",
    "\n",
    "<img src=\"http://mnemstudio.org/ai/path/images/map1a.gif\">\n",
    "\n",
    "In this image the start node is 2 and goal node is 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para inicializar la matriz de recompensas\n",
    "def init_reward_matrix(goal_state=5, initial_goal_reward=100):\n",
    "    \n",
    "    global R\n",
    "    # Matriz de recompensas\n",
    "    # cada fila es un estado, cada columna es un nodo de destino\n",
    "    # -1 marca un movimiento \"nulo\" y no se puede realizar\n",
    "    R = np.asmatrix([[-1, -1, -1, -1,  0, -1], # 0 can take action 4\n",
    "                     [-1, -1, -1,  0, -1,  0], # 1 can take action 3, 5\n",
    "                     [-1, -1, -1,  0, -1, -1], # 2 can take action 3\n",
    "                     [-1,  0,  0, -1,  0, -1], # 3 can take action 1, 4\n",
    "                     [ 0,  -1, -1, 0, -1,  0], # 4 can take action 0, 3, 5\n",
    "                     [-1,  0, -1, -1,  0,  -1] # 5 can take action 1, 4\n",
    "                    ])\n",
    "    \n",
    "    # añadir recompensa para movimientos de longitud 1 hacia la meta (por ejemplo, 100)\n",
    "    for (row_idx, col_idx), reward in np.ndenumerate(R):\n",
    "        if col_idx == goal_state and reward != -1:\n",
    "            R[row_idx, col_idx] = initial_goal_reward\n",
    "        \n",
    "        # Hacer posible/deseable el movimiento de la meta a la meta\n",
    "        if row_idx == goal_state and col_idx == goal_state:\n",
    "            R[row_idx, col_idx] = initial_goal_reward\n",
    "    \n",
    "    print(f\"Matriz de recompensas con estado objetivo {goal_state}: \\n {R}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener las acciones disponibles para un estado dado\n",
    "def available_actions(state):\n",
    "    current_state_row = R[state,]\n",
    "    actions = np.where(current_state_row != -1)[1]\n",
    "    return actions\n",
    "\n",
    "# Función para actualizar los valores Q\n",
    "def update(current_state, action, gamma):\n",
    "    # obtener el índice de columna del valor Q máximo \n",
    "    max_index = np.where(Q[action,] == np.max(Q[action,]))[1]\n",
    "\n",
    "    if max_index.shape[0] > 1:\n",
    "        # si hay varios, elegir al azar\n",
    "        max_index = int(np.random.choice(max_index, size=1))\n",
    "    else:\n",
    "        max_index = int(max_index)\n",
    "        \n",
    "    max_value = Q[action, max_index]\n",
    "\n",
    "    \"\"\" Fórmula de Q-learning\n",
    "    Omite la tasa de aprendizaje y el valor antiguo; efectivamente tiene una tasa de aprendizaje de 1 (ver fórmula a continuación)    \n",
    "    \"\"\"\n",
    "    Q[current_state, action] = R[current_state, action] + gamma * max_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/47fa1e5cf8cf75996a777c11c7b9445dc96d4637\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward matrix with goal_state 3: \n",
      " [[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1 100  -1   0]\n",
      " [ -1  -1  -1 100  -1  -1]\n",
      " [ -1   0   0 100   0  -1]\n",
      " [  0  -1  -1 100  -1   0]\n",
      " [ -1   0  -1  -1   0  -1]]\n",
      "\n",
      "Trained Q values:\n",
      "[[  0.   0.   0.   0. 400.   0.]\n",
      " [  0.   0.   0. 500.   0. 320.]\n",
      " [  0.   0.   0. 500.   0.   0.]\n",
      " [  0. 400. 400. 500. 400.   0.]\n",
      " [320.   0.   0. 500.   0. 320.]\n",
      " [  0. 400.   0.   0. 400.   0.]]\n",
      "\n",
      "\n",
      "Selected path: [1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "# Valores Q\n",
    "Q = np.asmatrix(np.zeros([6, 6]))\n",
    "\n",
    "Q_history =  []\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.8 # factor de descuento\n",
    "\n",
    "goal_state = int(input(\"Ingresa el estado objetivo (0-5) para entrenar: \"))\n",
    "init_reward_matrix(goal_state)\n",
    "\n",
    "\n",
    "# Entrenamiento: mejora iterativamente los valores Q para que indiquen acciones óptimas\n",
    "# Keep choosing a random state and a random action and updating Q-table\n",
    "def train(iterations=1000):\n",
    "    \n",
    "    for i in range(10000):\n",
    "        # estado inicial aleatorio\n",
    "        current_state = np.random.randint(int(Q.shape[0]))\n",
    "\n",
    "        # siguiente acción aleatoria (estado al que moverse)\n",
    "        available_act = available_actions(current_state)\n",
    "        action = int(np.random.choice(available_act, 1))\n",
    "        \n",
    "        # actualizar los valores Q\n",
    "        update(current_state, action, gamma)\n",
    "        np.set_printoptions(precision=3)\n",
    "\n",
    "        Q_history.append({\"Q\": np.copy(Q), \n",
    "                          \"state\": current_state, \n",
    "                          \"action\": action, \n",
    "                          \"reward\" : R[current_state, action]\n",
    "                        })\n",
    "        \n",
    "# ------------------------------\n",
    "# Entrenamiento\n",
    "train(10000)\n",
    "\n",
    "# Actualizar la matriz Q\n",
    "#update(initial_state, action, gamma)\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "print(f\"Trained Q values:\\n{Q}\\n\")\n",
    "\n",
    "#print(f\"Train Q values (normalised):\\n{Q / np.max(Q) * 100}\\n\")\n",
    "\n",
    "# ------------------------------\n",
    "# Testing / Testeando\n",
    "current_state = int(input(\"Enter starting state (0-5): \"))\n",
    "\n",
    "steps = [current_state]\n",
    "\n",
    "if current_state not in range(6) or goal_state not in range(6):\n",
    "    print(\"Error: Invalid states\" )\n",
    "    sys.exit(-1)\n",
    "\n",
    "num_timesteps = 10\n",
    "\n",
    "# Start episode\n",
    "# alternatively: \n",
    "# while current_state != goal_state:\n",
    "while num_timesteps > 0:\n",
    "    \n",
    "    # Explicación: np.where(condición) devuelve los índices (fila, columna) de la condición encontrada\n",
    "    # necesitamos el valor Q máximo -> el valor de columna más grande en la fila de estados actual\n",
    "    # esto se debe a que cada entrada en cada fila es la recompensa acumulativa por tomar la acción (columna)\n",
    "    # desde el estado actual (fila)\n",
    "    next_step_index = np.where(Q[current_state,] == np.max(Q[current_state,]))[1]\n",
    "\n",
    "    if next_step_index.shape[0] > 1:\n",
    "        next_step_index = int(np.random.choice(next_step_index, size=1))\n",
    "    else:\n",
    "        next_step_index = int(next_step_index)\n",
    "\n",
    "    steps.append(next_step_index)\n",
    "    current_state = next_step_index\n",
    "\n",
    "    num_timesteps -= 1\n",
    "    \n",
    "# Imprimir secuencia seleccionada de pasos\n",
    "print(f\"\\nSelected path: {steps}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0.   0.   0. 400.   0.]\n",
      " [  0.   0.   0. 500.   0. 320.]\n",
      " [  0.   0.   0. 500.   0.   0.]\n",
      " [  0. 400. 400. 500. 400.   0.]\n",
      " [320.   0.   0. 500.   0. 320.]\n",
      " [  0. 400.   0.   0. 400.   0.]]\n",
      "Iteration: 10000\n",
      "Random State: 5\n",
      "Random Action: 1\n",
      "Reward: 0\n"
     ]
    }
   ],
   "source": [
    "# Visualización del historial de entrenamiento\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_history(dicts):\n",
    "    '''Muestra cómo, durante el entrenamiento, se utilizan estados aleatorios con acciones aleatorias \n",
    "    para mejorar iterativamente los valores Q, que finalmente convergen'''\n",
    "    \n",
    "    for i, frame in enumerate(dicts):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['Q'])\n",
    "        print(f\"Iteration: {i + 1}\")\n",
    "        print(f\"Random State: {frame['state']}\")\n",
    "        print(f\"Random Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "              \n",
    "\n",
    "print_history(Q_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representación de la red\n",
    "network = r\"\"\" \n",
    "    1\n",
    "    | \\\n",
    "2 - 3  5\n",
    "    | /\n",
    "0 - 4\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "    1\n",
      "    | \\\n",
      "2 - 3  5\n",
      "    | /\n",
      "0 - 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(network)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
